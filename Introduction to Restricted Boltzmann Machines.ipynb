{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c66ec1a9-f447-48d1-a574-0a40e0115296"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Restricted Boltzmann Machines\n",
    "\n",
    "## Outline\n",
    "1. [What is a Boltzmann machine?](http://localhost:8888/notebooks/Introduction%20to%20Restricted%20Boltzmann%20Machines.ipynb#What-is-a-Boltzmann-machine?)\n",
    "2. [What problems can we solve with one?](http://localhost:8888/notebooks/Introduction%20to%20Restricted%20Boltzmann%20Machines.ipynb#What-problems-can-we-solve-with-one?)\n",
    "3. [Basic architecture](http://localhost:8888/notebooks/Introduction%20to%20Restricted%20Boltzmann%20Machines.ipynb#Basic-Architecture)\n",
    "4. [*Restricted* Boltzmann machines - how and why?](http://localhost:8888/notebooks/Introduction%20to%20Restricted%20Boltzmann%20Machines.ipynb#Restricted-Boltzmann-Machines---How-and-Why?)\n",
    "5. [Machinery](http://localhost:8888/notebooks/Introduction%20to%20Restricted%20Boltzmann%20Machines.ipynb#Restricted-Boltzmann-Machines---How-and-Why?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "ec87d30a-8fb1-4b1c-b663-2620bf47d2e7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is a Boltzmann machine?\n",
    "\n",
    "A Boltzmann machine is a type of neural network. It's made out of two types of neurons: *visible* and *hidden* units. Each unit has two states - \"on\" or \"off\" (sometimes also called \"activated\" or \"deactivated\").\n",
    "\n",
    "There is one layer of visible units, which represent the data we can feed in or read out. \n",
    "\n",
    "Examples:\n",
    " * Spin configurations\n",
    " * Pixel colour\n",
    " * A sentence\n",
    "\n",
    "The visible units connect to each other and to hidden units. There may be many layers of hidden units. The connections between each neuron are *weighted*. This allows the network to encode information about features in the dataset.\n",
    "\n",
    "Example: Spins nearby are somewhat correlated vs spins far apart are highly correlated.\n",
    "\n",
    "Can have multiple layers of hidden units - shallower layer works as visible layer for deeper layer. Allows identification of *higher level features*.\n",
    "\n",
    "Why not do this? We have to train the weights. We need to carefully adjust (more on this later) the weights to be able to accurately capture what's important in the data. The more layers, the more time-intensive this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c401ed0c-f3fd-4371-b25a-a9a18ad0c3de"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What problems can we solve with one?\n",
    "\n",
    "Boltzmann machines used to solve two kinds of problems: *learning* (interesting to us) and *search*. In a *learning* problem, make small updates to parameters which will allow us to generate *new* states.\n",
    "\n",
    "Boltzmann machines have been used to:\n",
    "  * classify handwriting - what letter/digit is present in this image? ![famous MNIST problem](mnist_digits.png)\n",
    "  * classify spin systems - is this the spin configuration of a para- or ferro-magnet?\n",
    "  * form \"deep belief networks\" - many hidden layers, cheap & greedy learning, represent hierarchy of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "46071c35-0b36-4b9b-b2f2-19864ccdae6a"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Basic Architecture\n",
    "\n",
    "We have a layer of *visible* units connected to *hidden* layer(s). \n",
    "\n",
    "![GBM](generalboltzmannmachine.png)\n",
    "\n",
    "Each *unit* (visible or hidden) receives a *total input* $z_i$:\n",
    "\n",
    "$$ z_i = b_i + \\sum_j s_j W_{ij} $$\n",
    "\n",
    "This should look familiar. $b_i$ is called the *bias* but we might recognize it as an external magnetic field. The $W_{ij}$ are the *weights* discussed above, and $s_j$ is the *state* of the $j$-th unit. Now this does look familiar! It's a classical Ising model. We might see the $W$ as $J$ - coupling constants. We will adjust their values to reproduce the training data we give the neural network. You might think of this as similar to \"training\" DFT to reproduce coupling constants based on observed electron density profiles.\n",
    "\n",
    "What's interesting about this is that once the network has been trained (we have good values for the weights, which reliably reproduce the training data), we can use the trained weights to generate *new* data. This has been done and may allow us to speed up Monte Carlo simulations of spin/hardcore-boson systems and Hamiltonians we can map to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The total \"energy\" of the system is \n",
    "\n",
    "$$ E = - \\sum_i s_i b_i - \\sum_{i < j} s_i s_j W_{ij} $$\n",
    "\n",
    "Note that each of the visible and hidden units may have a bias!\n",
    "\n",
    "For now we will assume the states $s_i$ are *binary* - they can be 0 or 1. This is actually not required, but makes reasoning about what's happening simpler.\n",
    "\n",
    "The probability to \"turn on\" a unit is determined by the so-called *logistic* or *sigmoid function*:\n",
    "\n",
    "$$ p_i(0 \\rightarrow 1) = \\frac{1}{1 - \\exp{(-z_i)}} $$\n",
    "\n",
    "Where $z_i$ is the input above. Look familiar? After enough updates, the system should reach a Boltzmann distribution (thus the name)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "a0e76b61-ae85-4a38-9172-238086913636"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why use hidden units?\n",
    "\n",
    "We cannot directly observe their state. This lets us learn features that are not encodable in *pairwise correlations* between the visible features. This is a very important distinction from a bare Ising model we are used to!\n",
    "\n",
    "We can learn binary features that capture higher-order relationships, because one hidden unit may be connected to many visible units (and vice-versa), in a sort of *n*-th nearest neighbor model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "85e9fc23-144a-4187-a5bb-c91c3acb53a7"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Restricted Boltzmann Machines - How and Why?\n",
    "\n",
    "A *restricted* Boltzmann machine is a special architecture. In such a case, there are *no* connections from visible units to visible units, or from hidden units to hidden units. *All* connections (and thus weights) are from visible units to hidden units.\n",
    "\n",
    "![RBM](generalvsrestricted.jpg)\n",
    "\n",
    "We can represent the weights as a 2d tensor (a matrix), which we'll initialize as random numbers drawn from a uniform box distribution.\n",
    "\n",
    "There's a bit of art to choosing the number of hidden units, which we can elide for now. Sometimes just one hidden unit will be enough, but let's hedge our bets and have a lot of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "6f72b489-9bff-49f5-b94a-044f15980337"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "using Distributions\n",
    "\n",
    "number_hidden  = 32\n",
    "number_visible = 64\n",
    "\n",
    "weight_width = 1/2*sqrt(number_hidden + number_visible)\n",
    "W            = rand(Uniform(-weight_width, weight_width), number_visible, number_hidden);\n",
    "hidden_bias  = rand(number_hidden);\n",
    "visible_bias = rand(number_visible);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e3b3ed18-a9f8-41ae-93ec-fafc4a729ed5"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why use restricted vs general?\n",
    "  * Given a specific visible vector (e.g. spin configuration), the hidden units are *independent* of each other\n",
    "  * This means all hidden units can be updated in parallel!\n",
    "  * Similarly, if we must sample visible units from hidden units, that can be done in parallel as well.\n",
    "\n",
    "Restricted Boltzmann machines (RBMs) can learn a good weight distribution much quicker than networks with more complicated connectivity.\n",
    "\n",
    "Now we need a way to *train* the network\n",
    "Must iteratively update the weights until we get a distribution that is \"good enough\" (not underfitted) but still \"flexible\" (not overfitted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Machinery\n",
    "\n",
    "Now we can discuss how to feed data into the RBM and train it.\n",
    "\n",
    "#### Training\n",
    "\n",
    "We want to search for the lowest cost (energy) solution.\n",
    "\n",
    "We need to update the weights using some scheme:\n",
    "\n",
    "$$ \\Delta W_{ij} = \\epsilon \\left(\\langle v_i h_j \\rangle_{data} - \\langle v_i h_j \\rangle_{model} \\right) $$\n",
    "\n",
    "There are a bunch of parameters here:\n",
    "  * $\\epsilon$ is the *learning rate*. It controls how much the weights should \"react\" to disparities\n",
    "  * $\\langle v_i h_j \\rangle_{data}$ is the association between the $i$-th visible unit and the $j$-th hidden unit sampled using the training data distribution. This is easy to compute.\n",
    "  * $\\langle v_i h_j \\rangle_{model}$ is the association between the $i$-th visible unit and the $j$-th hidden unit sampled using the model distribution. This is not easy to compute *in an unbiased way*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Computing the data sample\n",
    "Sampling from the data is easy *because* the hidden units have no connections to other hidden units. This means that, given a state of all the visible units, we can find the probability that the $j$-th hidden unit is set to 1 as:\n",
    "\n",
    "$$ p(h_j = 1 | v) = \\sigma\\left(b^h_j + \\sum_i W_{ij}v_i\\right) $$\n",
    "\n",
    "Where $b^h_j$ are the hidden biases. $\\sigma(x)$ is the *sigmoid* function defined above. $\\sum_i W_{ij}v_i$ is very easy to compute - this is just a matrix-vector multiplication. These multiplications can be performed very efficiently on modern computer hardware (GPUs). Similarly, it's very easy to get an unbiased sample of the *visible* units given the hidden ones:\n",
    "\n",
    "$$ p(v_i = 1 | h) = \\sigma\\left(b^v_i + \\sum_j W_{ij}h_j\\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "7ff04ee1-2860-43c0-89ff-8c82145d2685"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##### Computing the model sample\n",
    "If we are *not* given visible or hidden vectors (we'll need to compute the model association), this becomes much harder.\n",
    "\n",
    "Often this is done with [stochastic gradient descent](http://ufldl.stanford.edu/tutorial/supervised/OptimizationStochasticGradientDescent/) (SGD), a systematized way of escaping poor local minima (what we might call \"metastable states\").\n",
    "\n",
    "For other network types, like CNNs, we need to use backpropagation to update the weights efficiently. Another advantage of RBMs is that this is not necessary. We can \"cheat\" and use an approximation to SGD that has been shown to be very robust in practice: **constrastive divergence**.\n",
    "\n",
    "To sample the associations in the trained model so far, we can start with a random state of the visible units and perform Gibbs sampling for many time steps. How this works:\n",
    "  * Sample the hidden units from the visible units\n",
    "  * Sample the visible units from the hidden units\n",
    "  * Sample hidden from visible once again\n",
    "... for $k$ steps.\n",
    "\n",
    "How to pick the initial states? We pick a training vector, then \"time evolve\" the \"system\". In fact, computing the *true* gradient would still be hard, because there's a nasty term lurking which we will ignore. Instead we just compute the *difference* of two Kullback-Liebler divergences. We're using a *very* rough approximation to the true gradient that nevertheless works alright in practice. This is called $CD_k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "10b87dc1-bdfc-45f8-a5a5-343206f20af3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cd_k = 20 # set number of samples to take\n",
    "系    = 0.01\n",
    "\n",
    "sigmoid(x) = 1./(1. - exp(-x))\n",
    "relu(x)    = max(0, x)\n",
    "# force states to be 0 or 1 using ReLU\n",
    "function gibbs_sample(data, h_random, v_random)\n",
    "    h_probs    = W'*data .+ hidden_bias      |> sigmoid\n",
    "    h_states   = sign(-h_random .+ h_probs)  |> relu \n",
    "    v_probs    = W*h_probs .+ visible_bias   |> sigmoid \n",
    "    h_probs_1  = W'*v_probs .+ hidden_bias   |> sigmoid\n",
    "    h_states_1 = sign(-h_random .+ h_probs_1)|> relu\n",
    "    return h_probs, h_states, v_probs, h_probs_1, h_states_1\n",
    "end\n",
    "\n",
    "function update_parameters(input, W, hidden_bias, visible_bias, h_random, v_random)\n",
    "    h_probs_0, h_states_0, v_probs_0, h_probs_1, h_states_1 = gibbs_sample(input, h_random, v_random)\n",
    "    \n",
    "    cd_k_step_input = v_probs_0\n",
    "    for cdk_step in 2:cd_k\n",
    "        h_probs, h_states, v_probs, h_probs_1, h_states_1 = gibbs_sample(cd_k_step_input, h_random, v_random)\n",
    "        cd_k_step_input = v_probs\n",
    "    end\n",
    "    v_probs      = cd_k_step_input\n",
    "    \n",
    "    data_sample  = input * h_states_0'\n",
    "    model_sample = v_probs * h_probs_1'\n",
    "    \n",
    "    W            += 系*(data_sample - model_sample)\n",
    "    hidden_bias  += 系*mean(h_probs_0 - h_probs_1)\n",
    "    visible_bias += 系*mean(input - v_probs)\n",
    "    \n",
    "    loss = sqrt(mean((input-v_probs).^2))\n",
    "    return loss, W, hidden_bias, visible_bias\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "61579c6e-992a-4353-b4f5-1ab9c9da4e26"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Explaining some details\n",
    "We're using the ReLU (rectified linear unit) to map the probabilities to binary data. This is important to create an information bottleneck and force the RBM to \"focus\" on what is most important. It maps negative signs to 0 (since `sign` returns -1, 0, or 1).\n",
    "\n",
    "The loss function tells us how poorly we are doing. Remember, `v_probs` represents the visible unit probabilities after $k$ steps of sampling, so the closer they are to the true visible unit probabilities, the lower the loss should be. The goal is to minimize this loss.\n",
    "\n",
    "We will feed batched training samples into the training step, updating the weights and biases as we go. I've written a code to generate some simple Ising configurations using classical Monte Carlo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# generate a training data set of Ising configurations\n",
    "include(\"ising.jl\")\n",
    "\n",
    "num_epochs   = 100\n",
    "batch_size   = 50\n",
    "training_set = []\n",
    "T            = 1.0\n",
    "for i in 1:4\n",
    "    configurations_i = generate_Ising_configurations(1./T, isqrt(number_visible))\n",
    "    training_set_i   = configurations_i[10_001:end] #want equilibrated\n",
    "    append!(training_set, training_set_i)\n",
    "end\n",
    "num_batches = div(length(training_set), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "7fa4a395-0785-4f0b-9f26-34ab6e38ae49"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#train the network for the given set of configurations\n",
    "for epoch in 1:num_epochs\n",
    "    shuffle!(training_set)\n",
    "    for batch_id in 1:num_batches\n",
    "        batch_indices = (batch_id-1)*batch_size+1:batch_id*batch_size\n",
    "        #make a 2D matrix of examples\n",
    "        batch    = hcat(training_set[batch_indices]...)\n",
    "        #add noise to the bias\n",
    "        h_random = rand(number_hidden, batch_size)\n",
    "        v_random = rand(batch_size, number_visible)\n",
    "        #update weights and biases\n",
    "        loss, W, hidden_bias, visible_bias = update_parameters(batch, W, hidden_bias, visible_bias, h_random, v_random)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![weights](t_weights.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "e27291c5-06b0-431b-8a66-a6655c0ae8a3"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Further directions\n",
    "\n",
    "Here we've used binary data, but there are other choices. In fact, we can use any distribution for the neurons from the \"exponential family\", including:\n",
    " * Binary\n",
    " * Gaussian\n",
    " * Poisson\n",
    " * Binomial\n",
    " * ReLU\n",
    "\n",
    "This may allow us to generalize the RBMs to other problems. We can also use a pre-trained RBM to look at other, similar physics problems."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.6.0-dev",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  },
  "nbpresent": {
   "slides": {
    "24a250de-4839-4ddb-ae86-a00b88d9f7e4": {
     "id": "24a250de-4839-4ddb-ae86-a00b88d9f7e4",
     "prev": "ed927ebc-6fc4-4b1a-8999-17f98f3550fe",
     "regions": {
      "14929760-b1d7-412d-98f9-28e7fdb97c07": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7fa4a395-0785-4f0b-9f26-34ab6e38ae49",
        "part": "whole"
       },
       "id": "14929760-b1d7-412d-98f9-28e7fdb97c07"
      }
     }
    },
    "42868739-d8c6-49de-a128-0ba175de32a8": {
     "id": "42868739-d8c6-49de-a128-0ba175de32a8",
     "prev": "6a3ed14b-59a9-4b4b-b760-6350965aa806",
     "regions": {
      "fd080dfd-2870-43fe-958a-6e031b1dd27e": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c401ed0c-f3fd-4371-b25a-a9a18ad0c3de",
        "part": "whole"
       },
       "id": "fd080dfd-2870-43fe-958a-6e031b1dd27e"
      }
     }
    },
    "49901eb1-b9ef-4ee3-817f-f530bcd65fd5": {
     "id": "49901eb1-b9ef-4ee3-817f-f530bcd65fd5",
     "prev": "be520f91-488f-41a0-909f-f22aaec41372",
     "regions": {
      "329d1c57-dce4-44e5-b13b-67c026b4817f": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "a0e76b61-ae85-4a38-9172-238086913636",
        "part": "whole"
       },
       "id": "329d1c57-dce4-44e5-b13b-67c026b4817f"
      }
     }
    },
    "6a3ed14b-59a9-4b4b-b760-6350965aa806": {
     "id": "6a3ed14b-59a9-4b4b-b760-6350965aa806",
     "prev": "917beef6-7f8a-4555-8c22-602536f7a83f",
     "regions": {
      "3b08cc81-080e-4555-a80d-a0390a827185": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "ec87d30a-8fb1-4b1c-b663-2620bf47d2e7",
        "part": "whole"
       },
       "id": "3b08cc81-080e-4555-a80d-a0390a827185"
      }
     }
    },
    "70d249be-cd11-492e-96b2-7dd958db0133": {
     "id": "70d249be-cd11-492e-96b2-7dd958db0133",
     "prev": "dd05f424-7ffd-4d71-b464-a8737163c36a",
     "regions": {
      "2a00c9e6-276a-41e5-82d6-2b08e5497caa": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e3b3ed18-a9f8-41ae-93ec-fafc4a729ed5",
        "part": "whole"
       },
       "id": "2a00c9e6-276a-41e5-82d6-2b08e5497caa"
      }
     }
    },
    "76ad18b4-6856-453c-9a27-53c0f55aacae": {
     "id": "76ad18b4-6856-453c-9a27-53c0f55aacae",
     "prev": "24a250de-4839-4ddb-ae86-a00b88d9f7e4",
     "regions": {
      "347c0a08-1b90-4fb5-8461-58ab01fe4d7d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "e27291c5-06b0-431b-8a66-a6655c0ae8a3",
        "part": "whole"
       },
       "id": "347c0a08-1b90-4fb5-8461-58ab01fe4d7d"
      }
     }
    },
    "917beef6-7f8a-4555-8c22-602536f7a83f": {
     "id": "917beef6-7f8a-4555-8c22-602536f7a83f",
     "prev": null,
     "regions": {
      "68c9f4ab-dc0b-4abd-acad-9ee60b73ad5a": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "c66ec1a9-f447-48d1-a574-0a40e0115296",
        "part": "whole"
       },
       "id": "68c9f4ab-dc0b-4abd-acad-9ee60b73ad5a"
      }
     }
    },
    "946172f0-efaa-40fa-9228-61c3ffb6a209": {
     "id": "946172f0-efaa-40fa-9228-61c3ffb6a209",
     "prev": "ff35f1ff-4c09-4b72-a76e-53c805ead55c",
     "regions": {
      "03b53b16-ced0-49d8-9b2a-f33c8ac5e552": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "445d23fe-240f-4d6c-ac2d-c8629b7b835e",
        "part": "whole"
       },
       "id": "03b53b16-ced0-49d8-9b2a-f33c8ac5e552"
      }
     }
    },
    "be520f91-488f-41a0-909f-f22aaec41372": {
     "id": "be520f91-488f-41a0-909f-f22aaec41372",
     "prev": "42868739-d8c6-49de-a128-0ba175de32a8",
     "regions": {
      "0d138701-3594-414b-963c-3201201ccf6d": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "46071c35-0b36-4b9b-b2f2-19864ccdae6a",
        "part": "whole"
       },
       "id": "0d138701-3594-414b-963c-3201201ccf6d"
      }
     }
    },
    "dd05f424-7ffd-4d71-b464-a8737163c36a": {
     "id": "dd05f424-7ffd-4d71-b464-a8737163c36a",
     "prev": "946172f0-efaa-40fa-9228-61c3ffb6a209",
     "regions": {
      "561e5061-3886-4055-bf4d-cfbbc89c02e7": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "6f72b489-9bff-49f5-b94a-044f15980337",
        "part": "whole"
       },
       "id": "561e5061-3886-4055-bf4d-cfbbc89c02e7"
      }
     }
    },
    "ed927ebc-6fc4-4b1a-8999-17f98f3550fe": {
     "id": "ed927ebc-6fc4-4b1a-8999-17f98f3550fe",
     "prev": "f236222f-a28c-4a94-8e0c-9269b1ffd8b4",
     "regions": {
      "93652711-2a3e-473a-a4f6-e4a17e307e43": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "61579c6e-992a-4353-b4f5-1ab9c9da4e26",
        "part": "whole"
       },
       "id": "93652711-2a3e-473a-a4f6-e4a17e307e43"
      }
     }
    },
    "f236222f-a28c-4a94-8e0c-9269b1ffd8b4": {
     "id": "f236222f-a28c-4a94-8e0c-9269b1ffd8b4",
     "prev": "fbf20b8e-bc70-44a1-835f-d07d05a8fba9",
     "regions": {
      "572f41ee-1434-4e80-b524-17229ed2adb0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "10b87dc1-bdfc-45f8-a5a5-343206f20af3",
        "part": "whole"
       },
       "id": "572f41ee-1434-4e80-b524-17229ed2adb0"
      }
     }
    },
    "fbf20b8e-bc70-44a1-835f-d07d05a8fba9": {
     "id": "fbf20b8e-bc70-44a1-835f-d07d05a8fba9",
     "prev": "70d249be-cd11-492e-96b2-7dd958db0133",
     "regions": {
      "3043a518-e296-4ac1-846f-8968d25ebe53": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "7ff04ee1-2860-43c0-89ff-8c82145d2685",
        "part": "whole"
       },
       "id": "3043a518-e296-4ac1-846f-8968d25ebe53"
      }
     }
    },
    "ff35f1ff-4c09-4b72-a76e-53c805ead55c": {
     "id": "ff35f1ff-4c09-4b72-a76e-53c805ead55c",
     "prev": "49901eb1-b9ef-4ee3-817f-f530bcd65fd5",
     "regions": {
      "f20601f6-748d-4763-a092-eb6bb0042ba0": {
       "attrs": {
        "height": 0.8,
        "width": 0.8,
        "x": 0.1,
        "y": 0.1
       },
       "content": {
        "cell": "85e9fc23-144a-4187-a5bb-c91c3acb53a7",
        "part": "whole"
       },
       "id": "f20601f6-748d-4763-a092-eb6bb0042ba0"
      }
     }
    }
   },
   "themes": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
